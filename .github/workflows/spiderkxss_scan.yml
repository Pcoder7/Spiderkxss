name: Spider Kxss Running

on:
  workflow_dispatch: # Allows you to run this workflow manually

permissions:
  contents: write # We only need to read the repository contents

jobs:
  prepare-chunks:
    name: Prepare Chunks and Build Matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.build_matrix.outputs.matrix }}
      chunk_count: ${{ steps.build_matrix.outputs.chunk_count }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq
       
      - name: Create Chunks and Build Matrix
        id: build_matrix
        shell: bash
        run: |
          # --- 1. CONFIGURATION AND SETUP ---
          CHUNK_SIZE=100
          CHUNK_DIR="chunks"
          COMBINED_FILE="combined_targets.txt"

          # Ensure the combined file is empty before we start
          > "$COMBINED_FILE"

          # Check for the domains.txt file which lists the domains to process
          if [ ! -f "domains.txt" ]; then
            echo "::error::Input file 'domains.txt' not found in the repository root."
            exit 0
          fi

          echo "-> Starting download of filtered.txt files based on domains.txt..."

          # --- 2. DOWNLOAD AND COMBINE ALL INPUT FILES ---
          # Read domains.txt line-by-line, trim whitespace, and ignore comments/empty lines
          
          grep -v '^\s*#\|^\s*$' domains.txt | while IFS= read -r domain; do
            # The 'refs/heads/main' can be simplified to just 'main'
            URL="https://raw.githubusercontent.com/Pcoder7/ssrf-automation/main/results/${domain}/filtered.txt"
            echo "-> Downloading from: $URL"
            
            # Use wget: -q (quiet), -O - (output to stdout).
            # The '|| true' prevents the script from failing if one file doesn't exist (e.g., 404 error).
            wget -qO - "$URL" >> "$COMBINED_FILE" || echo "::warning::Failed to download for ${domain}. File may not exist."
          done
          
          # --- 3. VALIDATE DOWNLOADED CONTENT AND SPLIT INTO CHUNKS ---
          # Check if the combined file has any content before proceeding (-s checks for size > 0)
          if [ ! -s "$COMBINED_FILE" ]; then
            echo "::warning::No content was downloaded from any of the filtered.txt files. No work to do."
            echo "matrix=[]" >> "$GITHUB_OUTPUT"
            echo "chunk_count=0" >> "$GITHUB_OUTPUT"
            exit 0 # Exit successfully
          fi

          mkdir -p "$CHUNK_DIR"
          echo "-> Splitting '$COMBINED_FILE' into chunks of $CHUNK_SIZE lines..."
          split -l "$CHUNK_SIZE" -a 3 --numeric-suffixes=1 "$COMBINED_FILE" "$CHUNK_DIR/chunk_"
          
          echo "-> Chunk files created in '$CHUNK_DIR' directory:"
          ls -l "$CHUNK_DIR"
     
          CHUNK_COUNT=$(find "$CHUNK_DIR" -type f -name 'chunk_*' | wc -l)
          echo "-> Found $CHUNK_COUNT chunk files to process."
          
          # --- 4. BUILD THE MATRIX AND OUTPUT ---
          JSON_MATRIX=$(find "$CHUNK_DIR" -type f -name 'chunk_*' | jq -R -s -c 'split("\n") | map(select(length > 0))')
          
          echo "-> Verifying that the generated matrix is valid JSON..."
          echo "$JSON_MATRIX" | jq .
          
          echo "-> Matrix verification successful."
          echo "Final Matrix: $JSON_MATRIX"
          
          echo "matrix=$JSON_MATRIX" >> "$GITHUB_OUTPUT"
          echo "chunk_count=$CHUNK_COUNT" >> "$GITHUB_OUTPUT"
          
      - name: Upload Chunks as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: output-chunks
          path: chunks/
          retention-days: 1

  run-tools-in-parallel:
    needs: prepare-chunks
    if: ${{ needs.prepare-chunks.outputs.chunk_count > 0 }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
     
        chunk_file: ${{ fromJson(needs.prepare-chunks.outputs.matrix) }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          # Fetch full Git history so previous commits are available for comparison
          fetch-depth: 0   
     
      - name: Ensure go.mod exists (auto-init if missing)
        run: |
          if [ -f go.mod ]; then
            echo "go.mod exists at repo root"
            go env GOMOD
          else
            echo "go.mod not found — attempting to init module from git remote"
            origin=$(git config --get remote.origin.url || true)
            if [ -z "$origin" ]; then
              modpath="example.com/temp/module"
              echo "no git remote found; using temporary module path: $modpath"
            else
              # convert git@github.com:owner/repo.git -> https://github.com/owner/repo
              modpath=$(echo "$origin" \
                | sed -E 's#^git@([^:]+):#https://\1/#' \
                | sed -E 's#^ssh://##' \
                | sed -E 's#^https?://##' \
                | sed -E 's#\.git$##' \
                | sed -E 's#^#https://#')
              # sanitize resulting URL (make it module path form)
              modpath=$(echo "$modpath" | sed -E 's#https?://##')
              echo "derived module path: $modpath"
            fi

            # init module and tidy (may add go.sum)
            go mod init "$modpath"
            # tidy to populate go.sum (can fail if imports unreachable)
            go mod tidy || true
            echo "Created go.mod; CI will proceed but please commit go.mod/go.sum to repo for reproducible builds."
            go env GOMOD || true
          fi
          
          
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v4
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-

 
      - name: Download modules
        run: |
          go env GOMOD
          go mod download

      - name: Build binary
        run: |
          
          go build -o spiderkxss main.go
          ./spiderkxss --help || true
          
      - name: Download Chunks Artifact
        uses: actions/download-artifact@v4
        with:
          name: output-chunks
          path: chunks/
     
      - name: Debug Matrix Context
        run: |
          echo "matrix.chunk_file = ${{ matrix.chunk_file }}"
          
      - name: Run tools on Chunks
        id: run_tool
        shell: bash
        env:
          CHUNK_PATH: ${{ matrix.chunk_file }}
        run: |

          # verify chunk file exists
          if [ ! -f "$CHUNK_PATH" ]; then
            echo "::error:: Chunk file '$CHUNK_PATH' not found"
            ls -R .
            exit 0
          fi
          # derive basename for artifact naming
          BASENAME=$(basename "$CHUNK_PATH")
          echo "basename=$BASENAME" >> $GITHUB_OUTPUT
          OUTPUT_DIR="results"
          mkdir -p "$OUTPUT_DIR"
          OUTPUT_FILE="$OUTPUT_DIR/result_${BASENAME}"
          echo "Running httpx on $CHUNK_PATH → $OUTPUT_FILE"
          PUBLIC_IP=$(curl -s https://api.ipify.org) && echo "The public IP of this runner is: $PUBLIC_IP"
           
          cat "$CHUNK_PATH" | ./spiderkxss -c 40 -d 0.5-0.9 --rotate-agent 5-10 --ua-file user-agents.txt | anew -q "$OUTPUT_FILE"
          
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
      - name: Compute Safe Chunk Name
        id: safe_chunk
        run: |
          SAFE_CHUNK="$(echo "${{ matrix.chunk_file }}" | tr '/' '_')"
          echo "safe_chunk=$SAFE_CHUNK" >> $GITHUB_OUTPUT
          
      - name: Upload Individual Result Artifact
        uses: actions/upload-artifact@v4
        with:
          name: result-${{ steps.safe_chunk.outputs.safe_chunk }}
          path: ${{ steps.run_tool.outputs.output_file }}
          retention-days: 1

  aggregate-results:
    needs: run-tools-in-parallel
    if: always() # This ensures the aggregation job runs even if some nuclei jobs fail
    runs-on: ubuntu-latest
    steps:
      - name: Create Temporary Directory for Results
        run: mkdir -p temp-results

      - name: Download All Result Artifacts
        uses: actions/download-artifact@v4
        with:
          path: temp-results/
          pattern: result-*
          merge-multiple: true

      - name: Aggregate All Results into a Single File
        id: aggregate
        run: |
          echo "Aggregating all results..."
          # Combine all downloaded result files into one.
          # The `sort -u` command will also remove any duplicate findings.
          cat temp-results/* | sort -u > nuclei-final-results.txt
          echo "Final aggregated results created at nuclei-final-results.txt"
          echo "Total unique findings: $(wc -l < nuclei-final-results.txt)"
      
      - name: Upload Final Aggregated Results
        uses: actions/upload-artifact@v4
        with:
          name: final-results
          path: final-results.txt
          retention-days: 1

